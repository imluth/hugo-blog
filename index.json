[{"content":"Building a Robust Docker Monitoring Stack for Production As a DevOps engineer managing containerized environments, I\u0026rsquo;ve learned that proper observability isn\u0026rsquo;t just nice to haveâ€”it\u0026rsquo;s essential for maintaining system reliability and quickly resolving issues. After experimenting with various tools and configurations in my Docker lab, I\u0026rsquo;ve developed a comprehensive monitoring stack that I\u0026rsquo;m now deploying in production environments.\nIn this post, I\u0026rsquo;ll walk through my approach to building a complete monitoring solution for Docker environments using industry-standard tools and production-ready best practices.\nThe Monitoring Stack Architecture My monitoring solution combines metrics and logs collection into a unified observability platform. Here\u0026rsquo;s what we\u0026rsquo;ll be implementing:\nMetrics Collection \u0026amp; Storage\nPrometheus (time-series database) Node Exporter (host-level metrics) cAdvisor (container-level metrics) Log Aggregation\nLoki (log storage and querying) Promtail (log collector) Visualization\nGrafana (unified dashboard) Security \u0026amp; Access\nTraefik (reverse proxy with TLS) Basic authentication Prerequisites Before getting started, you\u0026rsquo;ll need:\nDocker and Docker Compose installed A server running Linux (I\u0026rsquo;m using Linode) A domain with DNS configured Traefik already set up as a reverse proxy with SSL support Implementation Details Docker Compose Configuration Let\u0026rsquo;s start with our complete docker-compose.yml configuration. This is the backbone of our monitoring stack:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 version: \u0026#34;3.3\u0026#34; services: prometheus: image: prom/prometheus:v2.48.1 container_name: prometheus restart: unless-stopped volumes: - ./prometheus:/etc/prometheus - prometheus_data:/prometheus command: - \u0026#39;--config.file=/etc/prometheus/prometheus.yml\u0026#39; - \u0026#39;--storage.tsdb.path=/prometheus\u0026#39; - \u0026#39;--web.console.libraries=/etc/prometheus/console_libraries\u0026#39; - \u0026#39;--web.console.templates=/etc/prometheus/consoles\u0026#39; - \u0026#39;--web.enable-lifecycle\u0026#39; deploy: resources: limits: cpus: \u0026#39;0.50\u0026#39; memory: 1G networks: - traefik-public - monitoring labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=traefik-public\u0026#34; - \u0026#34;traefik.http.routers.prometheus.rule=Host(`prometheus.looth.io`)\u0026#34; - \u0026#34;traefik.http.routers.prometheus.entrypoints=websecure\u0026#34; - \u0026#34;traefik.http.routers.prometheus.tls.certresolver=leresolver\u0026#34; - \u0026#34;traefik.http.services.prometheus.loadbalancer.server.port=9090\u0026#34; # Basic auth middleware - \u0026#34;traefik.http.routers.prometheus.middlewares=prometheus-auth\u0026#34; - \u0026#34;traefik.http.middlewares.prometheus-auth.basicauth.users=admin:\u0026lt;encryptedpassword\u0026gt;\u0026#34; node-exporter: image: prom/node-exporter:v1.7.0 container_name: node-exporter restart: unless-stopped volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command: - \u0026#39;--path.procfs=/host/proc\u0026#39; - \u0026#39;--path.rootfs=/rootfs\u0026#39; - \u0026#39;--path.sysfs=/host/sys\u0026#39; - \u0026#39;--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)\u0026#39; deploy: resources: limits: cpus: \u0026#39;0.10\u0026#39; memory: 128M networks: - monitoring expose: - 9100 cadvisor: image: gcr.io/cadvisor/cadvisor:v0.47.2 container_name: cadvisor restart: unless-stopped volumes: - /:/rootfs:ro - /var/run:/var/run:ro - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro - /dev/disk/:/dev/disk:ro deploy: resources: limits: cpus: \u0026#39;0.25\u0026#39; memory: 256M networks: - monitoring expose: - 8080 grafana: image: grafana/grafana:10.2.3 container_name: grafana restart: unless-stopped environment: - GF_SECURITY_ADMIN_USER=admin - GF_SECURITY_ADMIN_PASSWORD=secure_password_here - GF_USERS_ALLOW_SIGN_UP=false - GF_SERVER_ROOT_URL=https://grafana.looth.io volumes: - grafana_data:/var/lib/grafana deploy: resources: limits: cpus: \u0026#39;0.50\u0026#39; memory: 512M networks: - traefik-public - monitoring depends_on: - prometheus labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=traefik-public\u0026#34; - \u0026#34;traefik.http.routers.grafana.rule=Host(`grafana.looth.io`)\u0026#34; - \u0026#34;traefik.http.routers.grafana.entrypoints=websecure\u0026#34; - \u0026#34;traefik.http.routers.grafana.tls.certresolver=leresolver\u0026#34; - \u0026#34;traefik.http.services.grafana.loadbalancer.server.port=3000\u0026#34; loki: image: grafana/loki:2.9.5 container_name: loki restart: unless-stopped user: \u0026#34;0:0\u0026#34; # Run as root user to avoid permission issues volumes: - ./loki:/etc/loki - ./loki-data:/loki # Use bind mount instead of named volume command: -config.file=/etc/loki/loki-config.yaml deploy: resources: limits: cpus: \u0026#39;0.50\u0026#39; memory: 512M networks: - traefik-public - monitoring labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=traefik-public\u0026#34; - \u0026#34;traefik.http.routers.loki.rule=Host(`loki.looth.io`)\u0026#34; - \u0026#34;traefik.http.routers.loki.entrypoints=websecure\u0026#34; - \u0026#34;traefik.http.routers.loki.tls.certresolver=leresolver\u0026#34; - \u0026#34;traefik.http.services.loki.loadbalancer.server.port=3100\u0026#34; - \u0026#34;traefik.http.routers.loki.middlewares=loki-auth\u0026#34; - \u0026#34;traefik.http.middlewares.loki-auth.basicauth.users=admin:\u0026lt;encryptedpassword\u0026gt;\u0026#34; promtail: image: grafana/promtail:2.9.5 container_name: promtail restart: unless-stopped user: \u0026#34;0:0\u0026#34; # Run as root to avoid permission issues volumes: - ./promtail:/etc/promtail - /var/log:/var/log:ro - /var/lib/docker/containers:/var/lib/docker/containers:ro - /var/run/docker.sock:/var/run/docker.sock:ro command: -config.file=/etc/promtail/promtail-config.yaml deploy: resources: limits: cpus: \u0026#39;0.25\u0026#39; memory: 256M networks: - monitoring depends_on: - loki networks: traefik-public: external: true monitoring: driver: bridge volumes: prometheus_data: grafana_data: Essential Configuration Files Let\u0026rsquo;s look at the configuration for each component of our monitoring stack.\nPrometheus Configuration Create a prometheus.yml file in the ./prometheus directory:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 global: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] - job_name: \u0026#39;node-exporter\u0026#39; static_configs: - targets: [\u0026#39;node-exporter:9100\u0026#39;] - job_name: \u0026#39;cadvisor\u0026#39; static_configs: - targets: [\u0026#39;cadvisor:8080\u0026#39;] # Add scrape configs for your other services here - job_name: \u0026#39;traefik\u0026#39; static_configs: - targets: [\u0026#39;traefik:8080\u0026#39;] - job_name: \u0026#39;docker\u0026#39; static_configs: - targets: [\u0026#39;cadvisor:8080\u0026#39;] Loki Configuration Create a loki-config.yaml file in the ./loki directory:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 auth_enabled: false server: http_listen_port: 3100 ingester: lifecycler: address: 127.0.0.1 ring: kvstore: store: inmemory replication_factor: 1 final_sleep: 0s chunk_idle_period: 5m chunk_retain_period: 30s schema_config: configs: - from: 2020-01-01 store: boltdb-shipper object_store: filesystem schema: v11 index: prefix: index_ period: 24h storage_config: boltdb_shipper: active_index_directory: /loki/boltdb-shipper-active cache_location: /loki/boltdb-shipper-cache cache_ttl: 24h shared_store: filesystem filesystem: directory: /loki/chunks limits_config: enforce_metric_name: false reject_old_samples: true reject_old_samples_max_age: 168h Promtail Configuration Create a promtail-config.yaml file in the ./promtail directory:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 server: http_listen_port: 9080 positions: filename: /etc/promtail/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: docker static_configs: - targets: - localhost labels: job: docker __path__: /var/lib/docker/containers/*/*.log pipeline_stages: - docker: {} - labeldrop: - filename - stream Production-Ready Best Practices Throughout my testing and implementation, I\u0026rsquo;ve incorporated several production-ready practices that make this monitoring stack reliable and secure:\n1. Resource Management One of the most critical aspects of running a monitoring stack in production is proper resource management. I\u0026rsquo;ve learned from experience that without explicit limits, monitoring tools can sometimes consume excessive resources and impact the performance of your production services:\n1 2 3 4 5 deploy: resources: limits: cpus: \u0026#39;0.50\u0026#39; memory: 512M These limits prevent any single component from consuming too many resources and potentially bringing down the entire host. The values are based on my observations of typical resource usage in production environments.\n2. Network Segmentation The stack uses two separate networks:\ntraefik-public: An external network used for components that need to be publicly accessible through Traefik monitoring: An internal bridge network for components to communicate securely This approach follows the principle of least privilege by only exposing the necessary services, keeping the attack surface as small as possible.\n3. Data Persistence Both Prometheus and Grafana use named volumes to ensure data persistence across container restarts or upgrades:\n1 2 3 volumes: prometheus_data: grafana_data: For Loki, I\u0026rsquo;ve chosen a bind mount approach to make the data easier to back up and manage:\n1 2 volumes: - ./loki-data:/loki This strategy ensures you won\u0026rsquo;t lose historical metrics or dashboard configurations during maintenance or updates.\n4. Security Measures Several security practices are implemented in this stack:\nTLS Encryption: All public endpoints use HTTPS via Traefik\u0026rsquo;s Let\u0026rsquo;s Encrypt integration Basic Authentication: Sensitive dashboards (Prometheus, Loki) use basic auth to prevent unauthorized access Read-Only Mounts: All volume mounts use read-only access where possible Limited User Sign-up: Grafana is configured to prevent unauthorized user registration These measures ensure that your monitoring data remains secure and accessible only to authorized personnel.\n5. Container Best Practices Other Docker-specific best practices used throughout this stack:\nNamed Containers: Makes it easier to reference them in logs and commands Fixed Versions: Using specific image versions instead of latest for reproducibility Health Checks: Services implement health checks for better orchestration Restart Policies: All services are configured to restart automatically if they crash Setting Up Grafana Dashboards Once your stack is running, you\u0026rsquo;ll need to set up dashboards in Grafana for visualization.\n1. Add Data Sources First, add both Prometheus and Loki as data sources in Grafana:\nPrometheus:\nURL: http://prometheus:9090 Access: Server (default) Loki:\nURL: http://loki:3100 Access: Server (default) 2. Import Dashboard Templates Grafana has many pre-built dashboards you can import. Here are some recommended dashboard IDs that I\u0026rsquo;ve found particularly useful in production:\nNode Exporter Full: 1860 Docker Containers: 893 Traefik: 11462 3. Create a Custom Logs Dashboard For container logs, create a custom dashboard with these panels:\nAll Container Logs\nQuery: {container=~\u0026quot;.+\u0026quot;} Error Logs Across All Containers\nQuery: {container=~\u0026quot;.+\u0026quot;} |= \u0026quot;error\u0026quot; or {container=~\u0026quot;.+\u0026quot;} |= \u0026quot;ERROR\u0026quot; Logs by Container (using a variable)\nDashboard variable query: label_values(container) Panel query: {container=\u0026quot;$container\u0026quot;} Log Volume Over Time\nFor monitoring spikes in logging activity Here are some useful LogQL queries I frequently use in production:\n# All logs for a specific container {container=\u0026#34;traefik\u0026#34;} # Filter by HTTP status codes (for web services) {container=\u0026#34;traefik\u0026#34;} |~ \u0026#34;HTTP/1.1\\\\\u0026#34; (4|5)\\\\d\\\\d\u0026#34; # Find error messages {container=~\u0026#34;.+\u0026#34;} |= \u0026#34;error\u0026#34; or {container=~\u0026#34;.+\u0026#34;} |= \u0026#34;ERROR\u0026#34; # Find warnings {container=~\u0026#34;.+\u0026#34;} |= \u0026#34;WARN\u0026#34; or {container=~\u0026#34;.+\u0026#34;} |= \u0026#34;WARNING\u0026#34; Alerting and Notification While not covered in this basic setup, you can extend this monitoring stack with alerting capabilities:\nUse Prometheus AlertManager for metrics-based alerts Configure Grafana alerting for both logs and metrics Set up notification channels (email, Slack, PagerDuty, etc.) I\u0026rsquo;ll cover my alerting setup in a future post, as it deserves its own dedicated walkthrough.\nConclusion After several iterations in my lab environment, this monitoring stack has proven itself ready for production use. It provides complete visibility into containerized applications through both metrics and logs, all while following DevOps best practices for security, resource management, and data persistence.\nThe beauty of this approach is its modularityâ€”you can easily extend it with additional exporters, dashboards, or integrations as your monitoring needs evolve.\nBy combining Prometheus, Loki, and Grafana, you get a powerful monitoring solution that helps ensure the reliability and performance of your applications while making troubleshooting much more straightforward.\nNext Steps Based on my testing and production deployment, here are some future improvements I\u0026rsquo;m planning to implement:\nSetting up AlertManager for automated alerts based on resource thresholds Adding specialized exporters for MySQL, PostgreSQL, and Redis Implementing distributed tracing with Tempo to complement metrics and logs Exploring long-term storage solutions for metrics and logs retention Feel free to adapt this setup to your specific needs or reach out if you have any questions about implementing this in your own Docker environment!\n","permalink":"https://looth.io/posts/monitoring-stack-on-my-docker-lab/","summary":"\u003ch1 id=\"building-a-robust-docker-monitoring-stack-for-production\"\u003eBuilding a Robust Docker Monitoring Stack for Production\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"Grafana Overview\" loading=\"lazy\" src=\"/img/grafana-main.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAs a DevOps engineer managing containerized environments, I\u0026rsquo;ve learned that proper observability isn\u0026rsquo;t just nice to haveâ€”it\u0026rsquo;s essential for maintaining system reliability and quickly resolving issues. After experimenting with various tools and configurations in my Docker lab, I\u0026rsquo;ve developed a comprehensive monitoring stack that I\u0026rsquo;m now deploying in production environments.\u003c/p\u003e\n\u003cp\u003eIn this post, I\u0026rsquo;ll walk through my approach to building a complete monitoring solution for Docker environments using industry-standard tools and production-ready best practices.\u003c/p\u003e","title":"Building a Robust Docker Monitoring Stack"},{"content":"Infrastructure Overview My personal lab environment runs on a Linode-managed VM, serving as a Docker host for various containerized applications. Here\u0026rsquo;s the core infrastructure stack:\nHost Provider: Linode Docker-managed VM Domain: looth.io (managed through Porkbun) Container Management: Portainer Reverse Proxy: Traefik Version Control: GitHub Architecture Design The setup implements a GitOps-driven workflow, enabling automatic deployments through Portainer\u0026rsquo;s webhook integration with GitHub. This architecture supports rapid development and deployment of containerized applications.\nImplementation Details Development Workflow Local development and testing Containerization with Dockerfile and docker-compose.yml Code push to GitHub repository Automated deployment via Portainer webhooks Deployment Configuration The deployment process leverages Portainer\u0026rsquo;s stack functionality:\nStack configuration points to GitHub repository GitOps functionality enabled in Portainer Webhook URL from Portainer configured in GitHub repository Automatic deployment triggered on every push to the repository Why This Setup? This infrastructure serves as a learning platform for:\nContainer orchestration CI/CD pipeline implementation GitOps practices Microservices architecture DevOps methodologies Future Plans The environment is designed to support deployment and testing of various web applications and services, providing hands-on experience with:\nMicroservices development Container orchestration Automated deployment workflows Infrastructure as Code Application scaling Monitoring Conclusion This setup provides a robust foundation for experimenting with modern DevOps practices and containerized application deployment, making it an ideal environment for learning and testing new technologies.\n","permalink":"https://looth.io/posts/prod-ready-docker-lab/","summary":"\u003ch2 id=\"infrastructure-overview\"\u003eInfrastructure Overview\u003c/h2\u003e\n\u003cp\u003eMy personal lab environment runs on a Linode-managed VM, serving as a Docker host for various containerized applications. Here\u0026rsquo;s the core infrastructure stack:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHost Provider\u003c/strong\u003e: Linode Docker-managed VM\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDomain\u003c/strong\u003e: looth.io (managed through Porkbun)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eContainer Management\u003c/strong\u003e: Portainer\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReverse Proxy\u003c/strong\u003e: Traefik\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVersion Control\u003c/strong\u003e: GitHub\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"architecture-design\"\u003eArchitecture Design\u003c/h2\u003e\n\u003cp\u003eThe setup implements a GitOps-driven workflow, enabling automatic deployments through Portainer\u0026rsquo;s webhook integration with GitHub. This architecture supports rapid development and deployment of containerized applications.\u003c/p\u003e","title":"Building a Production-Ready Docker Environment with CI/CD Pipeline"},{"content":"Introduction I\u0026rsquo;m a System Administrator venturing into the world of DevOps, and this blog will document my learning journey. As someone who\u0026rsquo;s spent time managing traditional infrastructure, I\u0026rsquo;m excited to share my experiences as I explore modern DevOps practices and tools.\nWhat to Expect This blog will focus on practical tutorials and real-world examples covering:\nContainer orchestration with Docker and Kubernetes Automation with Ansible CI/CD pipelines using Jenkins Version control with GitHub Infrastructure as Code Cloud platforms and services I believe in learning by doing, so each post will include hands-on examples and practical implementations. Whether you\u0026rsquo;re a fellow SysAdmin looking to transition into DevOps or someone interested in automation and modern infrastructure, I hope you\u0026rsquo;ll find value in my documentation.\nStay tuned for my first technical post about setting up a basic CI/CD pipeline!\n","permalink":"https://looth.io/posts/welcome-post/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m a System Administrator venturing into the world of DevOps, and this blog will document my learning journey. As someone who\u0026rsquo;s spent time managing traditional infrastructure, I\u0026rsquo;m excited to share my experiences as I explore modern DevOps practices and tools.\u003c/p\u003e\n\u003ch2 id=\"what-to-expect\"\u003eWhat to Expect\u003c/h2\u003e\n\u003cp\u003eThis blog will focus on practical tutorials and real-world examples covering:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eContainer orchestration with Docker and Kubernetes\u003c/li\u003e\n\u003cli\u003eAutomation with Ansible\u003c/li\u003e\n\u003cli\u003eCI/CD pipelines using Jenkins\u003c/li\u003e\n\u003cli\u003eVersion control with GitHub\u003c/li\u003e\n\u003cli\u003eInfrastructure as Code\u003c/li\u003e\n\u003cli\u003eCloud platforms and services\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI believe in learning by doing, so each post will include hands-on examples and practical implementations. Whether you\u0026rsquo;re a fellow SysAdmin looking to transition into DevOps or someone interested in automation and modern infrastructure, I hope you\u0026rsquo;ll find value in my documentation.\u003c/p\u003e","title":"Welcome to My DevOps Journey"},{"content":"ðŸ‘‹ Hi, I\u0026rsquo;m Looth Ibrahim I\u0026rsquo;m a DevOps Engineer and System Administrator specializing in infrastructure automation, cloud technologies, and system optimization. With a strong foundation in Linux/Unix administration and a passion for DevOps practices, I work on bridging the gap between development and operations to create efficient, scalable systems.\nðŸ”§ Technical Skills Infrastructure as Code: Terraform, CloudFormation, Ansible Containerization \u0026amp; Orchestration: Docker, Kubernetes CI/CD: Jenkins, GitLab CI, GitHub Actions Cloud Platforms: AWS, Azure, GCP Operating Systems: Linux (RHEL, Ubuntu, CentOS), Unix Version Control: Git, GitHub, GitLab Monitoring \u0026amp; Logging: Prometheus, Grafana, ELK Stack Scripting: Bash, Python, PowerShell Security: InfoSec best practices, Security automation ðŸ’¼ What I Do Design and implement CI/CD pipelines Manage and automate cloud infrastructure Optimize system performance and reliability Implement security best practices Develop infrastructure automation solutions Support and maintain production environments Collaborate with development teams to improve deployment processes ðŸŒ± Interests I\u0026rsquo;m passionate about:\nInfrastructure automation Cloud-native technologies GitOps practices System security Open-source technologies Continuous learning and improvement ðŸ“« Connect With Me Feel free to reach out:\nGitHub: imluth LinkedIn: loothibrahim Twitter: @im_root Email: hello@looth.xyz ðŸš€ Recent Projects I regularly work on various projects involving:\nInfrastructure automation Container orchestration Deployment pipelines Monitoring solutions Security automation I\u0026rsquo;m always interested in collaborating on interesting projects or discussing DevOps practices. Feel free to reach out!\n","permalink":"https://looth.io/about/","summary":"\u003ch2 id=\"-hi-im-looth-ibrahim\"\u003eðŸ‘‹ Hi, I\u0026rsquo;m Looth Ibrahim\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m a DevOps Engineer and System Administrator specializing in infrastructure automation, cloud technologies, and system optimization. With a strong foundation in Linux/Unix administration and a passion for DevOps practices, I work on bridging the gap between development and operations to create efficient, scalable systems.\u003c/p\u003e\n\u003ch2 id=\"-technical-skills\"\u003eðŸ”§ Technical Skills\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eInfrastructure as Code\u003c/strong\u003e: Terraform, CloudFormation, Ansible\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eContainerization \u0026amp; Orchestration\u003c/strong\u003e: Docker, Kubernetes\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCI/CD\u003c/strong\u003e: Jenkins, GitLab CI, GitHub Actions\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCloud Platforms\u003c/strong\u003e: AWS, Azure, GCP\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOperating Systems\u003c/strong\u003e: Linux (RHEL, Ubuntu, CentOS), Unix\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVersion Control\u003c/strong\u003e: Git, GitHub, GitLab\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMonitoring \u0026amp; Logging\u003c/strong\u003e: Prometheus, Grafana, ELK Stack\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScripting\u003c/strong\u003e: Bash, Python, PowerShell\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSecurity\u003c/strong\u003e: InfoSec best practices, Security automation\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"-what-i-do\"\u003eðŸ’¼ What I Do\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDesign and implement CI/CD pipelines\u003c/li\u003e\n\u003cli\u003eManage and automate cloud infrastructure\u003c/li\u003e\n\u003cli\u003eOptimize system performance and reliability\u003c/li\u003e\n\u003cli\u003eImplement security best practices\u003c/li\u003e\n\u003cli\u003eDevelop infrastructure automation solutions\u003c/li\u003e\n\u003cli\u003eSupport and maintain production environments\u003c/li\u003e\n\u003cli\u003eCollaborate with development teams to improve deployment processes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"-interests\"\u003eðŸŒ± Interests\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;m passionate about:\u003c/p\u003e","title":"About Me"}]